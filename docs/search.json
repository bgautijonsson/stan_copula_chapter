[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Copulas",
    "section": "",
    "text": "Copulas provide a flexible way to model multivariate distributions by separating the marginal cumulative distribution functions from the dependence structure. This chapter introduces copulas in Stan, focusing on implementation techniques and practical examples.\n\n\nAccording to Sklar’s theorem (Sklar 1959), any multivariate distribution can be expressed in terms of its marginals and a copula that captures the dependence structure. Copulas are functions that join univariate marginal cumulative distribution functions to form multivariate distributions.\nFor a multivariate random variable \\(\\mathbf{X} = [X_1 \\cdots X_D]^\\top\\) with marginal cumulative distribution functions \\(F_i\\), the joint cumulative distribution function can be written as:\n\\[\nF_{\\mathbf{X}}(\\mathbf{x}) = C(F_1(x_1), \\ldots, F_D(x_D)) = \\Pr[X_1 \\leq x_1, \\ldots, X_D \\leq x_D]\n\\]\nwhere \\(C\\) is the copula function, \\(F_{\\mathbf{X}}\\) is the joint cumulative distribution function, and \\(F_i\\) are the marginal cumulative distribution functions. The copula function \\(C\\) must be a joint cumulative distribution function over the unit hypercube \\([0, 1]^D\\).\n\n\n\nThis section describes the general structure of copula models in Stan. The next sections will provide specific examples of copula implementations, but first, let’s understand the general pattern that separates the marginal distributions from the dependence structure.\nThe log density of a multivariate distribution using a copula can be written as:\n\\[\n\\log h(\\mathbf{x}) = \\log c\\left(u_1, \\dots, u_D \\vert \\boldsymbol{\\alpha}\\right) + \\sum_{i=1}^D \\log f_i(x_i \\vert \\boldsymbol{\\beta}_i)\n\\]\nwhere:\n\n\\(u_i = F_i(x_i \\vert \\boldsymbol{\\beta}_i)\\) are the probability integral transforms of the data\n\\(\\log c\\left(u_1, \\dots, u_D \\vert \\boldsymbol{\\alpha}\\right)\\) is the log density of the copula\n\\(\\sum_{i=1}^D \\log f_i(x_i \\vert \\boldsymbol{\\beta}_i)\\) is the sum of the log densities of the marginals\n\\(\\boldsymbol{\\alpha}\\) represents the parameters describing the parametric form of the copula\n\\(\\boldsymbol{\\beta}_i\\) represents the parameters describing the parametric form of the \\(i\\)-th marginal distribution\n\nThe implementation of copulas in Stan has two key requirements:\n\nBoth the probability density functions and cumulative distribution functions of the marginal distributions must be available\nA function that computes the log density of the copula for the transformed data must be implemented\n\nMost copula implementations in Stan follow a three-step process:\n\nAccumulate marginal log likelihoods: Calculate and add the log density of each marginal distribution to the target log density\nTransform to uniform variables: Apply the marginal CDFs to transform the data to uniform variables on the unit interval\nCalculate copula density: Compute the log density of the copula based on these uniform variables and add it to the target log density\n\nThis process is reflected in the general form of the log density shown above, where the first term represents the copula density and the second term represents the sum of marginal log densities.\nIn a way, we are always modeling with copulas, as the independence assumption can be viewed as a special case using the independence copula, where \\(\\log c(\\mathbf{u}) = 0\\), resulting in the familiar sum of marginal log densities. This perspective highlights that traditional independent modeling is just a specific case within the broader copula framework.\nMost parametric copula families include independence as a special case, either as a subset of their parameter space (e.g., when correlation parameters are zero) or as a limit when parameters approach specific values (e.g., when the dependence parameter approaches zero in Archimedean copulas).\n\n\n\nThe Gaussian copula is constructed using the multivariate normal distribution. For a \\(D\\)-dimensional random vector \\(\\mathbf{X}\\) with marginals \\(F_i\\), the log Gaussian copula density is given by:\n\\[\n\\begin{aligned}\n\\log c(\\mathbf{u}) &=\n-\\frac{1}{2} \\log |\\boldsymbol{\\Omega}| -\\frac{1}{2} \\mathbf{z}^\\top (\\boldsymbol{\\Omega}^{-1} - \\mathbf{I}) \\mathbf{z} \\\\\n& =\n-\\frac{1}{2} \\log |\\boldsymbol{\\Omega}| -\\frac{1}{2} \\mathbf{z}^\\top \\boldsymbol{\\Omega}^{-1} \\mathbf{z} + \\frac{1}{2} \\mathbf{z}^\\top \\mathbf{z} \\\\\n&= \\log \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\boldsymbol{\\Omega}) - \\log \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{I})\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{z} = [\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_D)]^\\top\\) are the inverse normal CDF transforms of the uniform marginals, \\(\\boldsymbol{\\Omega}\\) is the correlation matrix, and \\(\\mathbf{I}\\) is the identity matrix. The joint log density is then:\n\\[\n\\log h(\\mathbf{x}) = \\log c(F_1(x_1), \\ldots, F_D(x_D)) + \\sum_{i=1}^D \\log f_i(x_i)\n\\]\nFollowing the three-step process for implementing copulas in Stan:\n\nAccumulate marginal log likelihoods: The exponential log densities are added to the target in the line target += exponential_lpdf(y[n] | lambda)\nTransform to uniform variables: The exponential CDF transforms the data to uniform variables: exponential_cdf(y[n, d] | lambda[d])\nCalculate copula density: The transformed variables are converted to normal scale using inv_Phi and the multivariate normal log density is computed: z ~ multi_normal_cholesky(zeros, L_Omega)\n\nThe following example demonstrates a Gaussian copula with exponential marginal distributions. Note that while the copula is Gaussian, the marginals are exponential.\ndata {\n  int&lt;lower=0&gt; N;  // number of observations\n  int&lt;lower=0&gt; D;  // number of dimensions\n  vector&lt;lower=0&gt;[D] y[N];  // data\n}\n\ntransformed data {\n  vector[D] zeros = rep_vector(0, D);\n}\n\nparameters {\n  // Parameters for exponential marginal distributions\n  vector&lt;lower=0&gt;[D] lambda;  // rate parameters\n  \n  // Correlation matrix for Gaussian copula\n  cholesky_factor_corr[D] L_Omega;\n}\n\nmodel {\n  // Priors\n  lambda ~ gamma(2, 1);  // prior for rate parameters\n  L_Omega ~ lkj_corr_cholesky(2);\n  \n  // Likelihood using Gaussian copula with exponential marginals\n  for (n in 1:N) {\n    // Add exponential log density to target\n    target += exponential_lpdf(y[n] | lambda);\n    \n    vector[D] z;\n    for (d in 1:D) {\n      // Transform to uniform using exponential CDF\n      real u_d = exponential_cdf(y[n, d] | lambda[d]);\n      \n      // Transform to standard normal\n      z[d] = inv_Phi(u_d);\n    }\n    // Multivariate normal log density with correlation matrix\n    z ~ multi_normal_cholesky(zeros, L_Omega);\n  }\n}\n\ngenerated quantities {\n  // Optional: Recover correlation matrix from Cholesky factor\n  matrix[D, D] Omega = multiply_lower_tri_self_transpose(L_Omega);\n}\n\n\n\nCopulas offer several advantages in statistical modeling:\n\nFlexibility: They allow combining any marginal distributions with various dependence structures. For example:\n\nModeling financial returns with heavy-tailed marginals and complex dependence structures\nCombining different types of distributions (e.g., normal and gamma) in a single model\nCapturing asymmetric dependencies between variables, such as in financial markets where joint negative returns are more common than joint positive returns due to macro-events affecting multiple stocks simultaneously, while positive returns tend to be more idiosyncratic\nModeling different types of tail dependence in different parts of the distribution\n\nFactorability: The marginal distributions and dependence structure can be modeled separately, allowing for different prior knowledge about each component. This is similar to the common practice of factoring scale and correlation in multivariate normal priors.\nFor example, when modeling the survival times of two components in a system, we can separately specify exponential or gamma marginal distributions based on historical failure data for each component, and a Gaussian copula (or asymmetrical Archimedean copula) capturing how the failure of one component affects the other, making it easier to incorporate prior knowledge about each aspect independently.\nTail dependence: Different copulas can capture different types of tail dependence, which is crucial in applications like risk management and extreme value analysis where joint extreme scenarios need to be quantified.\nUniversal Framework: In a way, we are always modeling with copulas, as the independence assumption can be viewed as a special case using the independence copula. This perspective highlights that traditional independent modeling is just a specific case within the broader copula framework.\n\n\n\n\nWhen implementing copulas in Stan, several considerations should be kept in mind:\n\nComputational efficiency: The probability integral transform and inverse transform steps can be computationally intensive, especially for complex marginal distributions.\nParameter identifiability: Care must be taken to ensure that the parameters of the marginal distributions and the copula are identifiable.\nModel selection: The choice of copula family should be guided by the specific dependence structure of the data. For example:\n\nThe Gaussian copula may underestimate the probability of joint extreme events in financial data\nThe Student-t copula, while offering tail dependence, maintains symmetric tail behavior that may not match all applications\nArchimedean copulas can model asymmetric tail dependence but may be less flexible and harder to estimate in high dimensions\n\nNumerical stability: The transformations between different scales (original, uniform, and normal/Student-t/calculations using Archimedian copulas) require careful implementation to maintain numerical stability.\nSymmetry considerations: Many copula families exhibit strong symmetries that may not match the data:\n\nRadial symmetry: Some copulas (like Gaussian and Student-t) treat positive and negative extremes equally, which may not match financial data where joint negative returns are more common than joint positive returns\nExchangeability: Some copulas are invariant under permutations of their arguments, which can lead to unintuitive results when combined with inhomogeneous marginals. For example, when modeling time-to-event scenarios with different marginal distributions (e.g., exponential distributions with different parameters), perfect dependence in the copula does not imply simultaneous events. Instead, one event triggers the other at a later time corresponding to the same quantile, which can lead to incorrect modeling of joint events.\n\nTail dependence: Understanding and choosing appropriate tail dependence is crucial:\n\nThe upper (lower) tail dependence coefficient \\(\\lambda_U (\\lambda_L)\\) is the probability that one variable is extremely large (small) given that another is extremely large (small).\nDifferent copula families exhibit different tail dependence properties:\n\nSome copulas (like Gaussian) have zero tail dependence\nOthers can model symmetric tail dependence (\\(\\lambda_U = \\lambda_L\\))\nSome can capture asymmetric tail dependence (\\(\\lambda_U \\neq \\lambda_L\\))\nCertain copulas allow for tail dependence even with zero correlation\n\nThe choice of copula should be guided by the expected tail behavior in the application:\n\nFinancial data often requires modeling joint lower extreme events\nRisk management applications may need asymmetric tail dependence\nSome applications may require different tail behavior in different parts of the distribution\n\n\nHigh-dimensional modeling: As dimensionality increases:\n\nThe number of dependence parameters grows\nSome copula families become less flexible\nVine copulas or factor copulas may be more appropriate\n\n\n\n\n\nSeveral copula families are available for modeling different dependence structures in the correlation component:\n\nGaussian copula: Based on the multivariate normal distribution, offering symmetric dependence\nStudent-t copula: Based on the multivariate Student-t distribution, providing more flexibility in tail dependence than the Gaussian copula\nArchimedean copulas: A class of copulas defined through generator functions, including:\n\nClayton copula: Stronger lower tail dependence\nGumbel copula: Stronger upper tail dependence\nFrank copula: Symmetric dependence\n\nVine copulas: A flexible approach for modeling high-dimensional dependencies by decomposing the joint distribution into a series of bivariate copulas"
  },
  {
    "objectID": "index.html#what-are-copulas",
    "href": "index.html#what-are-copulas",
    "title": "Copulas",
    "section": "",
    "text": "According to Sklar’s theorem (Sklar 1959), any multivariate distribution can be expressed in terms of its marginals and a copula that captures the dependence structure. Copulas are functions that join univariate marginal cumulative distribution functions to form multivariate distributions.\nFor a multivariate random variable \\(\\mathbf{X} = [X_1 \\cdots X_D]^\\top\\) with marginal cumulative distribution functions \\(F_i\\), the joint cumulative distribution function can be written as:\n\\[\nF_{\\mathbf{X}}(\\mathbf{x}) = C(F_1(x_1), \\ldots, F_D(x_D)) = \\Pr[X_1 \\leq x_1, \\ldots, X_D \\leq x_D]\n\\]\nwhere \\(C\\) is the copula function, \\(F_{\\mathbf{X}}\\) is the joint cumulative distribution function, and \\(F_i\\) are the marginal cumulative distribution functions. The copula function \\(C\\) must be a joint cumulative distribution function over the unit hypercube \\([0, 1]^D\\)."
  },
  {
    "objectID": "index.html#general-structure-of-copula-models-in-stan",
    "href": "index.html#general-structure-of-copula-models-in-stan",
    "title": "Copulas",
    "section": "",
    "text": "This section describes the general structure of copula models in Stan. The next sections will provide specific examples of copula implementations, but first, let’s understand the general pattern that separates the marginal distributions from the dependence structure.\nThe log density of a multivariate distribution using a copula can be written as:\n\\[\n\\log h(\\mathbf{x}) = \\log c\\left(u_1, \\dots, u_D \\vert \\boldsymbol{\\alpha}\\right) + \\sum_{i=1}^D \\log f_i(x_i \\vert \\boldsymbol{\\beta}_i)\n\\]\nwhere:\n\n\\(u_i = F_i(x_i \\vert \\boldsymbol{\\beta}_i)\\) are the probability integral transforms of the data\n\\(\\log c\\left(u_1, \\dots, u_D \\vert \\boldsymbol{\\alpha}\\right)\\) is the log density of the copula\n\\(\\sum_{i=1}^D \\log f_i(x_i \\vert \\boldsymbol{\\beta}_i)\\) is the sum of the log densities of the marginals\n\\(\\boldsymbol{\\alpha}\\) represents the parameters describing the parametric form of the copula\n\\(\\boldsymbol{\\beta}_i\\) represents the parameters describing the parametric form of the \\(i\\)-th marginal distribution\n\nThe implementation of copulas in Stan has two key requirements:\n\nBoth the probability density functions and cumulative distribution functions of the marginal distributions must be available\nA function that computes the log density of the copula for the transformed data must be implemented\n\nMost copula implementations in Stan follow a three-step process:\n\nAccumulate marginal log likelihoods: Calculate and add the log density of each marginal distribution to the target log density\nTransform to uniform variables: Apply the marginal CDFs to transform the data to uniform variables on the unit interval\nCalculate copula density: Compute the log density of the copula based on these uniform variables and add it to the target log density\n\nThis process is reflected in the general form of the log density shown above, where the first term represents the copula density and the second term represents the sum of marginal log densities.\nIn a way, we are always modeling with copulas, as the independence assumption can be viewed as a special case using the independence copula, where \\(\\log c(\\mathbf{u}) = 0\\), resulting in the familiar sum of marginal log densities. This perspective highlights that traditional independent modeling is just a specific case within the broader copula framework.\nMost parametric copula families include independence as a special case, either as a subset of their parameter space (e.g., when correlation parameters are zero) or as a limit when parameters approach specific values (e.g., when the dependence parameter approaches zero in Archimedean copulas)."
  },
  {
    "objectID": "index.html#gaussian-copula-example",
    "href": "index.html#gaussian-copula-example",
    "title": "Copulas",
    "section": "",
    "text": "The Gaussian copula is constructed using the multivariate normal distribution. For a \\(D\\)-dimensional random vector \\(\\mathbf{X}\\) with marginals \\(F_i\\), the log Gaussian copula density is given by:\n\\[\n\\begin{aligned}\n\\log c(\\mathbf{u}) &=\n-\\frac{1}{2} \\log |\\boldsymbol{\\Omega}| -\\frac{1}{2} \\mathbf{z}^\\top (\\boldsymbol{\\Omega}^{-1} - \\mathbf{I}) \\mathbf{z} \\\\\n& =\n-\\frac{1}{2} \\log |\\boldsymbol{\\Omega}| -\\frac{1}{2} \\mathbf{z}^\\top \\boldsymbol{\\Omega}^{-1} \\mathbf{z} + \\frac{1}{2} \\mathbf{z}^\\top \\mathbf{z} \\\\\n&= \\log \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\boldsymbol{\\Omega}) - \\log \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{I})\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{z} = [\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_D)]^\\top\\) are the inverse normal CDF transforms of the uniform marginals, \\(\\boldsymbol{\\Omega}\\) is the correlation matrix, and \\(\\mathbf{I}\\) is the identity matrix. The joint log density is then:\n\\[\n\\log h(\\mathbf{x}) = \\log c(F_1(x_1), \\ldots, F_D(x_D)) + \\sum_{i=1}^D \\log f_i(x_i)\n\\]\nFollowing the three-step process for implementing copulas in Stan:\n\nAccumulate marginal log likelihoods: The exponential log densities are added to the target in the line target += exponential_lpdf(y[n] | lambda)\nTransform to uniform variables: The exponential CDF transforms the data to uniform variables: exponential_cdf(y[n, d] | lambda[d])\nCalculate copula density: The transformed variables are converted to normal scale using inv_Phi and the multivariate normal log density is computed: z ~ multi_normal_cholesky(zeros, L_Omega)\n\nThe following example demonstrates a Gaussian copula with exponential marginal distributions. Note that while the copula is Gaussian, the marginals are exponential.\ndata {\n  int&lt;lower=0&gt; N;  // number of observations\n  int&lt;lower=0&gt; D;  // number of dimensions\n  vector&lt;lower=0&gt;[D] y[N];  // data\n}\n\ntransformed data {\n  vector[D] zeros = rep_vector(0, D);\n}\n\nparameters {\n  // Parameters for exponential marginal distributions\n  vector&lt;lower=0&gt;[D] lambda;  // rate parameters\n  \n  // Correlation matrix for Gaussian copula\n  cholesky_factor_corr[D] L_Omega;\n}\n\nmodel {\n  // Priors\n  lambda ~ gamma(2, 1);  // prior for rate parameters\n  L_Omega ~ lkj_corr_cholesky(2);\n  \n  // Likelihood using Gaussian copula with exponential marginals\n  for (n in 1:N) {\n    // Add exponential log density to target\n    target += exponential_lpdf(y[n] | lambda);\n    \n    vector[D] z;\n    for (d in 1:D) {\n      // Transform to uniform using exponential CDF\n      real u_d = exponential_cdf(y[n, d] | lambda[d]);\n      \n      // Transform to standard normal\n      z[d] = inv_Phi(u_d);\n    }\n    // Multivariate normal log density with correlation matrix\n    z ~ multi_normal_cholesky(zeros, L_Omega);\n  }\n}\n\ngenerated quantities {\n  // Optional: Recover correlation matrix from Cholesky factor\n  matrix[D, D] Omega = multiply_lower_tri_self_transpose(L_Omega);\n}"
  },
  {
    "objectID": "index.html#advantages-of-copulas",
    "href": "index.html#advantages-of-copulas",
    "title": "Copulas",
    "section": "",
    "text": "Copulas offer several advantages in statistical modeling:\n\nFlexibility: They allow combining any marginal distributions with various dependence structures. For example:\n\nModeling financial returns with heavy-tailed marginals and complex dependence structures\nCombining different types of distributions (e.g., normal and gamma) in a single model\nCapturing asymmetric dependencies between variables, such as in financial markets where joint negative returns are more common than joint positive returns due to macro-events affecting multiple stocks simultaneously, while positive returns tend to be more idiosyncratic\nModeling different types of tail dependence in different parts of the distribution\n\nFactorability: The marginal distributions and dependence structure can be modeled separately, allowing for different prior knowledge about each component. This is similar to the common practice of factoring scale and correlation in multivariate normal priors.\nFor example, when modeling the survival times of two components in a system, we can separately specify exponential or gamma marginal distributions based on historical failure data for each component, and a Gaussian copula (or asymmetrical Archimedean copula) capturing how the failure of one component affects the other, making it easier to incorporate prior knowledge about each aspect independently.\nTail dependence: Different copulas can capture different types of tail dependence, which is crucial in applications like risk management and extreme value analysis where joint extreme scenarios need to be quantified.\nUniversal Framework: In a way, we are always modeling with copulas, as the independence assumption can be viewed as a special case using the independence copula. This perspective highlights that traditional independent modeling is just a specific case within the broader copula framework."
  },
  {
    "objectID": "index.html#common-pitfalls-and-considerations",
    "href": "index.html#common-pitfalls-and-considerations",
    "title": "Copulas",
    "section": "",
    "text": "When implementing copulas in Stan, several considerations should be kept in mind:\n\nComputational efficiency: The probability integral transform and inverse transform steps can be computationally intensive, especially for complex marginal distributions.\nParameter identifiability: Care must be taken to ensure that the parameters of the marginal distributions and the copula are identifiable.\nModel selection: The choice of copula family should be guided by the specific dependence structure of the data. For example:\n\nThe Gaussian copula may underestimate the probability of joint extreme events in financial data\nThe Student-t copula, while offering tail dependence, maintains symmetric tail behavior that may not match all applications\nArchimedean copulas can model asymmetric tail dependence but may be less flexible and harder to estimate in high dimensions\n\nNumerical stability: The transformations between different scales (original, uniform, and normal/Student-t/calculations using Archimedian copulas) require careful implementation to maintain numerical stability.\nSymmetry considerations: Many copula families exhibit strong symmetries that may not match the data:\n\nRadial symmetry: Some copulas (like Gaussian and Student-t) treat positive and negative extremes equally, which may not match financial data where joint negative returns are more common than joint positive returns\nExchangeability: Some copulas are invariant under permutations of their arguments, which can lead to unintuitive results when combined with inhomogeneous marginals. For example, when modeling time-to-event scenarios with different marginal distributions (e.g., exponential distributions with different parameters), perfect dependence in the copula does not imply simultaneous events. Instead, one event triggers the other at a later time corresponding to the same quantile, which can lead to incorrect modeling of joint events.\n\nTail dependence: Understanding and choosing appropriate tail dependence is crucial:\n\nThe upper (lower) tail dependence coefficient \\(\\lambda_U (\\lambda_L)\\) is the probability that one variable is extremely large (small) given that another is extremely large (small).\nDifferent copula families exhibit different tail dependence properties:\n\nSome copulas (like Gaussian) have zero tail dependence\nOthers can model symmetric tail dependence (\\(\\lambda_U = \\lambda_L\\))\nSome can capture asymmetric tail dependence (\\(\\lambda_U \\neq \\lambda_L\\))\nCertain copulas allow for tail dependence even with zero correlation\n\nThe choice of copula should be guided by the expected tail behavior in the application:\n\nFinancial data often requires modeling joint lower extreme events\nRisk management applications may need asymmetric tail dependence\nSome applications may require different tail behavior in different parts of the distribution\n\n\nHigh-dimensional modeling: As dimensionality increases:\n\nThe number of dependence parameters grows\nSome copula families become less flexible\nVine copulas or factor copulas may be more appropriate"
  },
  {
    "objectID": "index.html#common-copula-families",
    "href": "index.html#common-copula-families",
    "title": "Copulas",
    "section": "",
    "text": "Several copula families are available for modeling different dependence structures in the correlation component:\n\nGaussian copula: Based on the multivariate normal distribution, offering symmetric dependence\nStudent-t copula: Based on the multivariate Student-t distribution, providing more flexibility in tail dependence than the Gaussian copula\nArchimedean copulas: A class of copulas defined through generator functions, including:\n\nClayton copula: Stronger lower tail dependence\nGumbel copula: Stronger upper tail dependence\nFrank copula: Symmetric dependence\n\nVine copulas: A flexible approach for modeling high-dimensional dependencies by decomposing the joint distribution into a series of bivariate copulas"
  }
]